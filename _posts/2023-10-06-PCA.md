---
title: Mathematical understanding about PCA
author: euphoria0-0
date: 2023-10-06 23:00:00 +0800
categories: [Machine Learning]
tags: [Machine Learning]
toc: true
math: true
comments: true
---

# PCA의 수학적 이해

그림의 예시처럼 2차원 데이터를 1차원으로 축소한다고 가정해봅시다. 여기서 축소한 공간의 방향을 $\mathbf{u}_1$라 하고, 이는 방향을 나타내는 벡터이므로 편의상 단위벡터로 가정($\mathbf{u}_1^T\mathbf{u}_1=1$)합니다. 2차원 데이터 $\mathbf{x}_i$를 공간 $\mathbf{u}_1$에 직교투영하면 투영된 데이터는 
$$
\mathbf{u}_1^T\mathbf{x}_i
$$
입니다.

## 1. 투영된 데이터의 분산을 최대화


투영된 데이터들의 분산은 
$$
\frac{1}{N}\sum_{i=1}^N||\mathbf{u}_1^T\mathbf{x}_i - \mathbf{u}_1^T\bar{\mathbf{x}}||^2=\mathbf{u}_1^T\mathbf{S}\mathbf{u}_1
$$
로 정의할 수 있으며, 여기서 공분산 행렬 $\mathbf{S}$는 
$$
\mathbf{S}=\frac{1}{N}\sum_{i=1}^N(\mathbf{x}_i - \bar{\mathbf{x}})(\mathbf{x}_i - \bar{\mathbf{x}})^T
$$
로 정의될 수 있습니다.

따라서 이를 최대화한다면 $\mathbf{u}_1^T\mathbf{u}_1=1$ 라는 조건 하에 Lagrangian multipliers를 이용하여 
$$
\mathbf{u}_1^T\mathbf{S}\mathbf{u}_1+\lambda(1-\mathbf{u}_1^T\mathbf{u}_1)
$$
를 최대화하면 됩니다.

$\mathbf{u}_1$에 대해서 미분을 하면 
$$
\mathbf{S}\mathbf{u}_1=\lambda\mathbf{u}_1
$$
이 되고, 이는 공분산행렬 $\mathbf{S}$ 의 __고유벡터(eigenvector)__ 임을 알 수 있습니다.

양변의 왼쪽에 $\mathbf{u}_1^T$를 곱하면 $\mathbf{u}_1^T\mathbf{S}\mathbf{u}_1=\lambda$가 됨.
여기서 분산 $\mathbf{u}_1^T\mathbf{S}\mathbf{u}_1$를 최대화해야하므로 고유값(eigenvalue) 중 최댓값을 선택하고 이 값을 가지는 고유벡터를 $\mathbf{u}_1$로 정의합니다.

이를 __제1 주성분(Principal Component)__ 이라 함. 2차원 이상의 차원을 찾고자 하면 현재까지 구한 주성분을 뺀 뒤에(직교), 주성분을 남은 고유값 크기 순서대로 가진 고유벡터로 정의하며, 줄이고 싶은 차원 수 만큼 고유벡터를 찾으면 됩니다.



## 2. 투영된 데이터와 원래 데이터의 오차를 최소화

투영된 데이터와 원래 데이터의 오차는 $$J=\frac{1}{N}\sum_{i=1}^N\|\mathbf{x}_i-\tilde{\mathbf{x}}_i\|^2$$로 정의할 수 있습니다. 이를 계산하여 최소화하기 위해 기저 벡터(basis vector) 이용합니다.

완전 정규직교하는 D차원의 기저 벡터(basis vector)를 $\{\mathbf{u}_n\}, n=1,\dots,D$라 하면, $\mathbf{u}_n^T\mathbf{u}_m=$(스칼라 값)를 만족합니다.

이들의 선형 결합을 이용하면 각 데이터 포인트들은 
$$
\mathbf{x}_i=\sum_{n=1}^N\alpha_{in}\mathbf{u}_n=\sum_{n=1}^N\left(\mathbf{x}_i^T\mathbf{u}_n\right)\mathbf{u}_n
$$
이 됩니다. 이는 기존 $\mathbf{x}_i=\{x_{i1},\dots,x_{iD}\}$가 $\{\alpha_{i1},\dots,\alpha_{iD}\}$로 대체되는 것입니다. 

여기서 $\mathbf{x}_i$와 $\mathbf{u}_j$를 내적(inner product)하여 풀면 $\alpha_{jn}=\mathbf{x}_i^T\mathbf{u}_j$가 됩니다. 여기서 차원을 M만큼 줄이는 것을 목표로 하면, 데이터를 처음 M개의 기저 벡터를 이용해 M차원 선형 부분공간(subspace)으로 투영할 수 있습니다: -
$$
\tilde{\mathbf{x}}_i=\sum_{n=1}^Mz_{in}\mathbf{u}_n+\sum_{n=M+1}^Db_n\mathbf{u}_n.
$$


이를 위의 평균 거리에 대입하면 찾고자 하는 벡터 $\mathbf{u}_n$ 뿐만 아니라 추가적인 변수 $\{z_{in}\}, \{b_n\}$에 대해서도 최소화해야 합니다.


## Reference

1. Bishop, C. M. (2006). *Pattern recognition and machine learning*. springer.